#This script classifies article abstracts with LSA

#(c) Richard Kunert

if(!require(textstem)){install.packages('textstem')} #
library(textstem) #lemmatization (rather than stupid stemming)

#source custom functions
source('RK_fun.R')

##############################################################################################
# GLOBAL VARIABLES
nDim = 140 #how many dimensions to get for the LSA

wd = 'C:\\Users\\Richard\\Desktop\\R\\HU_text_classification'

setwd(wd)

abstracts = read.delim(paste(wd, 'abstracts', "ALL.txt", sep = '\\'),
                       sep = '\t', row.names = NULL, header = T, na.strings = c("", ""), quote = NULL)

#Lemmatize the corpus
abstracts$abstract = lemmatize_strings(abstracts$abstract)

#Turn text into vector representation
texts_vectorized = LSA_fun(abstracts$abstract, nDim = nDim)#Latent Semantic Analysis
texts_vectorized = LDA_fun(abstracts$abstract, nDim = nDim)#Latent Dirichlet Allocation

#determine the testing and training subsets
classified_idx = c(which(abstracts$included == 1), which(abstracts$included == 0))#reduce full sample to those cases in which human raters could determine label
testing_size = 100#number of articles in testing set
testing_sample_idx = sample(classified_idx, testing_size)
training_sample_idx = classified_idx[!(classified_idx %in% testing_sample_idx)]

#balance the training data set
data_training = balance_fun(texts = texts_vectorized[training_sample_idx,],
                            classes = abstracts$included[training_sample_idx],
                            balance_algo = 'SMOTE')

#try out which SVM classifier works the best
diagnostics = SVM_fun(texts_train = data_training$texts,
                      texts_test = texts_vectorized[testing_sample_idx,], 
                      classes_train = data_training$classes,
                      classes_test = abstracts$included[testing_sample_idx])


###################################################################################
#Classification with LSA typicality value

# #The typicality value is just a correlation between the inclusion centroid's semantic dimension values 
# # and the semantic dimension values of the astract
# inclusion_centroid = colMeans(space$dk[abstracts$included == 1,], na.rm = T)
# corr_centroid = unlist(lapply(1:nrow(abstracts), function(x) cor(data.frame(space$dk[x,], inclusion_centroid))[1,2]))
# 
# #One could now plot the distribution of typicality values, i.e. the Pearson correlation coefficients
# #start off with a histogram only made for extracting values. The interest is not in looking at p
# p = ggplot(data = data.frame(x = corr_centroid,
#                              included = as.factor(abstracts$included)),
#            aes(x = x, fill = included, linetype = included)) +
#   geom_histogram(position = 'identity', alpha = 0.5, color = 'black')
# # extract relevant variables from the plot object to a new data frame
# # your grouping variable 'included' is named 'group' in the plot object
# df <- ggplot_build(p)$data[[1]][ , c("xmin", "y", "group")]
# #get the factor levels interpretable
# df$group[df$group == 4] = 'unclassified'
# df$group[df$group == 3] = 'included'
# df$group[df$group == 2] = 'unsure'
# df$group[df$group == 1] = 'excluded'
# #plot
# ggplot(data = df, aes(x = xmin, y = y, color = factor(group))) +
#   geom_step(size = 1.5)
# 
# #An ROC curve (true positive rate versus false positive rate) would also be nice
# ROC = matrix(NA, nrow= 100, ncol = 3)
# counter = 0
# for(r in seq(-1, 1, length.out = 100)){#for each possible cut-off value
#   counter = counter + 1
#   TPR = sum(corr_centroid[abstracts$included == 1] > r, na.rm = T)/sum(abstracts$included == 1, na.rm = T)
#   FPR = sum(corr_centroid[abstracts$included == 0] > r, na.rm = T)/sum(abstracts$included == 0, na.rm = T)
#   ACC = sum(corr_centroid[abstracts$included == 1] > r, na.rm = T,#true positive
#             corr_centroid[abstracts$included == 0] < r, na.rm = T)/#true negative
#     sum(abstracts$included == 0, abstracts$included == 1, na.rm = T)
#   ROC[counter,] = c(FPR, TPR, ACC)
# }
# plot(ROC[,1:2], xlab = 'False positive rate', ylab = 'True positive rate')
# plot(ROC[,3], xlab = 'idx', ylab = 'Accuracy', ylim = c(0,1))
# max(ROC[,3])
# 
# #Finally, explore some concrete values
# # min(corr_centroid[abstracts$included == 1], na.rm = T)
# # min(corr_centroid[abstracts$included == 0], na.rm = T)
# # min(corr_centroid[abstracts$included == 0.5], na.rm = T)
# # 
# # max(corr_centroid[abstracts$included == 1], na.rm = T)
# # max(corr_centroid[abstracts$included == 0], na.rm = T)
# # max(corr_centroid[abstracts$included == 0.5], na.rm = T)
# 
# #proportion of to-be-excluded abstracts which can be excluded without excluding any to-be-included abstracts
# sum(corr_centroid[abstracts$included == 0] < 
#       min(corr_centroid[abstracts$included == 1], na.rm = T), na.rm = T)/
#   sum(abstracts$included == 0, na.rm = T)
# 
# rejection_cutoff = min(corr_centroid[abstracts$included == 1], na.rm = T)#no false inclusions
# rejection_cutoff = 0.25#only value with accuracy above 90% according to grid search
# 
# true_rejections = sum(corr_centroid[abstracts$included == 0] < rejection_cutoff, na.rm = T)
# false_inclusions = sum(corr_centroid[abstracts$included == 0] > rejection_cutoff, na.rm = T)
# true_inclusions = sum(corr_centroid[abstracts$included == 1] > rejection_cutoff, na.rm = T)
# false_rejections = sum(corr_centroid[abstracts$included == 1] < rejection_cutoff, na.rm = T)
# 
# accuracy = sum(true_rejections, true_inclusions)/sum(true_rejections, true_inclusions, false_rejections, false_inclusions)
# accuracy


